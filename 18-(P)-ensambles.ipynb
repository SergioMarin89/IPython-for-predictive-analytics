{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensambles de modelos\n",
    "===\n",
    "\n",
    "**Juan David Velásquez Henao**  \n",
    "jdvelasq@unal.edu.co   \n",
    "Universidad Nacional de Colombia, Sede Medellín  \n",
    "Facultad de Minas  \n",
    "Medellín, Colombia\n",
    "\n",
    "---\n",
    "\n",
    "Haga click [aquí](https://github.com/jdvelasq/predictive-analytics/blob/master/21-R-ensambles.ipynb) para acceder a la última versión online.\n",
    "\n",
    "Haga click [aquí](http://nbviewer.jupyter.org/github/jdvelasq/predictive-analytics/blob/master/21-R-ensambles.ipynb) para ver la última versión online en `nbviewer`. \n",
    "\n",
    "---\n",
    "[Licencia](https://github.com/jdvelasq/predictive-analytics/blob/master/LICENSE)  \n",
    "[Readme](https://github.com/jdvelasq/predictive-analytics/blob/master/readme.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del problema real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desea construir un esquema de pronóstico que permita aprovechar la información capturada por muchos modelos diferentes construidos sobre los mismos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del problema en términos de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se tienen múltiples pronósticos obtenidos por diferentes modelos para los mismos datos, es mejor simplemente seleccionar el mejor modelo y descartar la demás información, o se pueden aprovechar dichos pronósticos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ensamble es un tipo de modelo que permite la combinación de varios modelos de predicción para obtener un solo pronóstico basado en los pronósticos individuales de cada modelo. En la figura siguiente se presenta un esquema ilustrativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/combiner.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clave de la operación de esta metodología se basa en la diversidad, la cual puede obtenerse de diferentes formas:\n",
    "\n",
    "* Variando los datos de entrenamiento: se usa el mismo modelo en todos los casos, pero para cada uno de ellos se usa una muestra de entrenamiento obtenida por boostraping; así cada modelo tiene parámetros diferentes ya que fue estimado sobre una muestra diferente.\n",
    "\n",
    "\n",
    "* Variando la configuración del modelo: se usan exactamente los mismos datos de entrenamiento, pero sobre diferentes modelos obtenidos variando su configuración; por ejemplo, el mismo modelo pero con diferente configuración (entradas usadas, complejidad, etc). Inclusive se pueden utilizar distintos modelos.\n",
    "\n",
    "\n",
    "* Una combinación de los dos anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El combinador es un mecanismo que obtiene un único pronóstico a partir de los pronósticos individuales de cada modelo. Para problemas de clasificación, la conbinación se hace por votación. En problemas de regresión, mediante promedio simple, promedio combinado o, inclusive, regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferentes metodologías se han desarrollado sobre este concepto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging (Bootstrap aggregation)\n",
    "\n",
    "En esta metodología, la diversidad se obtiene al entrenar un mismo modelo sobre diferentes conjuntos de entrenamiento usando bootstraping. La combinación se hace por votación para problemas de clasificación y por promedio para problemas numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## install.packages(\"ipred\")\n",
    "## library(ipred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           \n",
       "credit_pred  No Yes\n",
       "        No  699   0\n",
       "        Yes   1 300"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## crea 25 arboles de decisión\n",
    "## credit <- read.csv(\"data/credit.csv\")\n",
    "## credit$default <- factor(credit$default, labels=c(\"No\", \"Yes\"))\n",
    "## mybag <- bagging(default ~ ., data = credit, nbagg = 25)\n",
    "## credit_pred <- predict(mybag, credit)\n",
    "## table(credit_pred, credit$default)\n",
    "\n",
    "##            \n",
    "## credit_pred  No Yes\n",
    "##         No  699   0\n",
    "##         Yes   1 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Bagged CART \n",
       "\n",
       "1000 samples\n",
       "  20 predictor\n",
       "   2 classes: 'No', 'Yes' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... \n",
       "Resampling results:\n",
       "\n",
       "  Accuracy  Kappa  \n",
       "  0.746     0.36038\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Para evaluar realmente la precisión del modelo\n",
    "## se usa crossvalidation\n",
    "## library(caret)\n",
    "## set.seed(300)\n",
    "## ctrl <- trainControl(method = \"cv\", number = 10)\n",
    "## train(default ~ ., data = credit, method = \"treebag\", trControl = ctrl)\n",
    "# note que tiene un desempeño muy similar al metodo C5.0\n",
    "\n",
    "## Loading required package: lattice\n",
    "## Loading required package: ggplot2\n",
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoosting (Adaptive Boosting)\n",
    "\n",
    "En este caso, los conjuntos de dato son diseñados especificamente para generar modelos complementarios. De forma simplificada, el algoritmo procede de la siguiente forma:\n",
    "\n",
    "* Paso 1: Se construye un clasificador sobre todos los datos de entrenamiento.\n",
    "\n",
    "\n",
    "* Paso 2: Se construye un nuevo conjunto de datos con los ejemplos mal clasificados (o una porción de ellos).\n",
    "\n",
    "\n",
    "* Paso 3: se construye un nuevo clasificador con los datos obtenidos en el paso 2.\n",
    "\n",
    "\n",
    "* Paso 4: Se retorna al Paso 2.\n",
    "\n",
    "\n",
    "El proceso itera hasta que se alcanza una precisión requerida o el número máximo de clasificadores permitidos en el ensamble. La ponderación se realiza dando más peso a los modelos con mejor desempeño, de tal forma, que el desempeño es, al menos, similar al del mejor clasificador obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: rpart\n",
      "Loading required package: foreach\n",
      "Loading required package: doParallel\n",
      "Loading required package: iterators\n",
      "Loading required package: parallel\n",
      "\n",
      "Attaching package: ‘adabag’\n",
      "\n",
      "The following object is masked from ‘package:ipred’:\n",
      "\n",
      "    bagging\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "               Observed Class\n",
       "Predicted Class  No Yes\n",
       "            No  700   0\n",
       "            Yes   0 300"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## install.packages(\"adabag\")\n",
    "## library(adabag)\n",
    "## set.seed(300)\n",
    "## m_adaboost <- boosting(default ~ ., data = credit)\n",
    "## p_adaboost <- predict(m_adaboost, credit)\n",
    "## p_adaboost$confusion\n",
    "\n",
    "## Loading required package: rpart\n",
    "## Loading required package: foreach\n",
    "## Loading required package: doParallel\n",
    "## Loading required package: iterators\n",
    "## Loading required package: parallel\n",
    "## \n",
    "## Attaching package: ‘adabag’\n",
    "## \n",
    "## The following object is masked from ‘package:ipred’:\n",
    "## \n",
    "##     bagging\n",
    "## \n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  1 Mon Mar  5 20:27:54 2018 \n",
      "i:  2 Mon Mar  5 20:28:31 2018 \n",
      "i:  3 Mon Mar  5 20:29:06 2018 \n",
      "i:  4 Mon Mar  5 20:29:42 2018 \n",
      "i:  5 Mon Mar  5 20:30:18 2018 \n",
      "i:  6 Mon Mar  5 20:30:55 2018 \n",
      "i:  7 Mon Mar  5 20:31:31 2018 \n",
      "i:  8 Mon Mar  5 20:32:06 2018 \n",
      "i:  9 Mon Mar  5 20:32:42 2018 \n",
      "i:  10 Mon Mar  5 20:33:15 2018 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "               Observed Class\n",
       "Predicted Class  No Yes\n",
       "            No  594 153\n",
       "            Yes 106 147"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## verificación usando CV\n",
    "## set.seed(300)\n",
    "## adaboost_cv <- boosting.cv(default ~ ., data = credit)\n",
    "## adaboost_cv$confusion\n",
    "\n",
    "## i:  1 Mon Mar  5 20:27:54 2018 \n",
    "## i:  2 Mon Mar  5 20:28:31 2018 \n",
    "## i:  3 Mon Mar  5 20:29:06 2018 \n",
    "## i:  4 Mon Mar  5 20:29:42 2018 \n",
    "## i:  5 Mon Mar  5 20:30:18 2018 \n",
    "## i:  6 Mon Mar  5 20:30:55 2018 \n",
    "## i:  7 Mon Mar  5 20:31:31 2018 \n",
    "## i:  8 Mon Mar  5 20:32:06 2018 \n",
    "## i:  9 Mon Mar  5 20:32:42 2018 \n",
    "## i:  10 Mon Mar  5 20:33:15 2018 \n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: grid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "            value     ASE     z Pr(>|z|)\n",
       "Unweighted 0.3544 0.03237 10.95 6.68e-28\n",
       "Weighted   0.3544 0.03237 10.95 6.68e-28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## install.packages(\"vcd\")\n",
    "## library(vcd)\n",
    "## Kappa(adaboost_cv$confusion)\n",
    "\n",
    "## Loading required package: grid\n",
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "\n",
    "En este método se combinan el bagging con la selección aleatoria de caracteríticas para aumetar la diversidad. La salida del modelo se obtiene por votación. Cada ejemplo que no es considerado durante el entrenamiento es usado como parte del conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "randomForest 4.6-12\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Attaching package: ‘randomForest’\n",
      "\n",
      "The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    margin\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       " randomForest(formula = default ~ ., data = credit) \n",
       "               Type of random forest: classification\n",
       "                     Number of trees: 500\n",
       "No. of variables tried at each split: 4\n",
       "\n",
       "        OOB estimate of  error rate: 22.7%\n",
       "Confusion matrix:\n",
       "     No Yes class.error\n",
       "No  649  51  0.07285714\n",
       "Yes 176 124  0.58666667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## install.packages(\"randomForest\")\n",
    "## library(randomForest)\n",
    "## set.seed(300)\n",
    "## por defecto crea un ensamble de 500 arboles\n",
    "## que usan la raíz cuadrada de la cantidad\n",
    "## de atributos presentes en el conjunto de entrenamiento\n",
    "## rf <- randomForest(default ~ ., data = credit)\n",
    "## rf\n",
    "## en la salida OOB se refiere a out-of-bag error rate\n",
    "\n",
    "## randomForest 4.6-12\n",
    "## Type rfNews() to see new features/changes/bug fixes.\n",
    "## \n",
    "## Attaching package: ‘randomForest’\n",
    "## \n",
    "## The following object is masked from ‘package:ggplot2’:\n",
    "## \n",
    "##     margin\n",
    "## \n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "1000 samples\n",
       "  20 predictor\n",
       "   2 classes: 'No', 'Yes' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 10 times) \n",
       "Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  Accuracy  Kappa    \n",
       "   2    0.7191    0.0990317\n",
       "   4    0.7491    0.2813675\n",
       "   8    0.7546    0.3294882\n",
       "  16    0.7562    0.3527924\n",
       "\n",
       "Kappa was used to select the optimal model using the largest value.\n",
       "The final value used for the model was mtry = 16."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## A continuación se comparan los resultados de \n",
    "## random forest (rf) con C5.0 \n",
    "## library(caret)\n",
    "\n",
    "## parametros de control\n",
    "## ctrl <- trainControl(method = \"repeatedcv\",\n",
    "##                      number = 10, \n",
    "##                      repeats = 10)\n",
    "\n",
    "## malla de parametros a considerar\n",
    "## grid_rf <- expand.grid(.mtry = c(2, 4, 8, 16))\n",
    "\n",
    "## set.seed(300)\n",
    "## m_rf <- train(default ~ ., \n",
    "##               data = credit, \n",
    "##               method = \"rf\",\n",
    "##               metric = \"Kappa\", \n",
    "##               trControl = ctrl,\n",
    "##               tuneGrid = grid_rf)\n",
    "## m_rf \n",
    "\n",
    "## Random Forest \n",
    "## \n",
    "## 1000 samples\n",
    "##   20 predictor\n",
    "##    2 classes: 'No', 'Yes' \n",
    "## \n",
    "## No pre-processing\n",
    "## Resampling: Cross-Validated (10 fold, repeated 10 times) \n",
    "## Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... \n",
    "## Resampling results across tuning parameters:\n",
    "## \n",
    "##   mtry  Accuracy  Kappa    \n",
    "##    2    0.7191    0.0990317\n",
    "##    4    0.7491    0.2813675\n",
    "##    8    0.7546    0.3294882\n",
    "##   16    0.7562    0.3527924\n",
    "## \n",
    "## Kappa was used to select the optimal model using the largest value.\n",
    "## The final value used for the model was mtry = 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in Ops.factor(x$winnow):\n",
      "“‘!’ not meaningful for factors”"
     ]
    },
    {
     "data": {
      "text/plain": [
       "C5.0 \n",
       "\n",
       "1000 samples\n",
       "  20 predictor\n",
       "   2 classes: 'No', 'Yes' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 10 times) \n",
       "Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  trials  Accuracy  Kappa    \n",
       "  10      0.7404    0.3389905\n",
       "  20      0.7476    0.3557574\n",
       "  30      0.7502    0.3623037\n",
       "  40      0.7509    0.3645426\n",
       "\n",
       "Tuning parameter 'model' was held constant at a value of tree\n",
       "Tuning\n",
       " parameter 'winnow' was held constant at a value of FALSE\n",
       "Kappa was used to select the optimal model using the largest value.\n",
       "The final values used for the model were trials = 40, model = tree and winnow\n",
       " = FALSE."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## comparación con un boosted tree\n",
    "## grid_c50 <- expand.grid(.model = \"tree\",\n",
    "##                         .trials = c(10, 20, 30, 40),\n",
    "##                         .winnow = \"FALSE\")\n",
    "## set.seed(300)\n",
    "## m_c50 <- train(default ~ ., \n",
    "##                data = credit, \n",
    "##                method = \"C5.0\",\n",
    "##                metric = \"Kappa\", \n",
    "##                trControl = ctrl,\n",
    "##                tuneGrid = grid_c50)\n",
    "## m_c50\n",
    "\n",
    "## Warning message in Ops.factor(x$winnow):\n",
    "## “‘!’ not meaningful for factors”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio.--** Cuál de los dos modelos anteriores fue mejor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensambles de modelos\n",
    "===\n",
    "\n",
    "**Juan David Velásquez Henao**  \n",
    "jdvelasq@unal.edu.co   \n",
    "Universidad Nacional de Colombia, Sede Medellín  \n",
    "Facultad de Minas  \n",
    "Medellín, Colombia\n",
    "\n",
    "---\n",
    "\n",
    "Haga click [aquí](https://github.com/jdvelasq/predictive-analytics/blob/master/21-R-ensambles.ipynb) para acceder a la última versión online.\n",
    "\n",
    "Haga click [aquí](http://nbviewer.jupyter.org/github/jdvelasq/predictive-analytics/blob/master/21-R-ensambles.ipynb) para ver la última versión online en `nbviewer`. \n",
    "\n",
    "---\n",
    "[Licencia](https://github.com/jdvelasq/predictive-analytics/blob/master/LICENSE)  \n",
    "[Readme](https://github.com/jdvelasq/predictive-analytics/blob/master/readme.md)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nteract": {
   "version": "0.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
